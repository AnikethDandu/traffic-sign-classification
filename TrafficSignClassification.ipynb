{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrafficSignClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSRLdpCmW16juXEzBMHmJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnikethDandu/traffic-sign-classification/blob/main/TrafficSignClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPE0QpmtL9y2"
      },
      "source": [
        "# **Traffic Sign Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aD9pIBhyupU"
      },
      "source": [
        "## **Google Drive Dataset Import**\n",
        "*Make sure to follow dataset download instructions on [Github](https://github.com/AnikethDandu/traffic-sign-classification)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgAG3K6-u43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp -r /content/gdrive/My\\ Drive/ColabNotebooks/Data/ traffic_sign_images.zip\n",
        "!unzip traffic_sign_images.zip/traffic_sign_images.zip\n",
        "!rm -r traffic_sign_images.zip/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDRPiq7M-y2"
      },
      "source": [
        "## **Import Libraries**\n",
        "Python standard libraries. If there are problems with torch, run the following command\n",
        "```\n",
        "!pip install torch\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ5biOPK3mdf"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCycgL3iNHHZ"
      },
      "source": [
        "## **Main Script Variable Initialization**\n",
        "Initializes variables for the main script. It includes hyperparameters for the network, a standard image size, the datatsets and dataloaders, and the path to the training data directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCnIB_Hm47cd"
      },
      "source": [
        "# Network hyperparameters\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Tuple for desired image size\n",
        "image_size = (-1, 3, 50, 50)\n",
        "\n",
        "# Training and evaluation Datasets and DataLoaders\n",
        "training_dataset = None\n",
        "testing_dataset = None\n",
        "train_dataloader = None\n",
        "test_dataloader = None\n",
        "\n",
        "# Path to class sorted training data directories\n",
        "train_path = 'traffic_sign_images/Train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJrsqdxNKPH"
      },
      "source": [
        "## **Convolutional Neural Network**\n",
        "A CNN class with four 2D Convolutional layers and two Linear layers. The ReLU function is appliead after each layer. The max pooling function is applied after each Convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zvNd8X31UI"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolution Neural Network class\n",
        "  Extends torch.nn.Module base network initialization\n",
        "  Overrides torch.nn.Module forward method\n",
        "\n",
        "  PUBLIC METHODS:\n",
        "    - forward(self, x)\n",
        "\n",
        "  INSTANCE VARIABLES:\n",
        "    - PADDING_SIZE\n",
        "    - KERNEL_SIZE\n",
        "    - STRIDE\n",
        "    - POOL_SIZE\n",
        "    - conv1\n",
        "    - conv2\n",
        "    - conv3\n",
        "    - conv4\n",
        "    - fc1\n",
        "    - fc2\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes network layers for input image of size 32x32x3\n",
        "\n",
        "    :var PADDING_SIZE: size of padding applied to all sides of input matrix to preserve input volume\n",
        "    :type: int\n",
        "    :var KERNEL_SIZE: size of feature extraction filter\n",
        "    :type: int\n",
        "    :var STRIDE: pixel translation length during convolution operation\n",
        "    :type: int\n",
        "    :var POOL_SIZE: size of pooled feature map\n",
        "    :type: int\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    self.PADDING_SIZE = 1\n",
        "    self.KERNEL_SIZE = 3\n",
        "    self.STRIDE = 1\n",
        "    self.POOL_SIZE = 2\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv2 = nn.Conv2d(32, 64, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv3 = nn.Conv2d(64, 128, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv4 = nn.Conv2d(128, 256, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.fc1 = nn.Linear(2304, 512)\n",
        "    self.fc2 = nn.Linear(512, 43)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Passes input matrix through network convolutional and linear layers while \n",
        "    applying pooling and ReLU function\n",
        "\n",
        "    :param x: input matrix\n",
        "    :type: torch.tensor\n",
        "    :return: class output matrix\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv4(x)), self.POOL_SIZE)\n",
        "    x = x.flatten(start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Fq0ceYNOGS"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLxUiPzfNi2n"
      },
      "source": [
        "### **Custom Dataset Class**\n",
        "The class subclasses the PyTorch Dataset class to read the training and evaluation csv and iterate over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKU0VKEt7laU"
      },
      "source": [
        "class TrafficSignDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Custom Dataset class\n",
        "  Subclasses torch.utils.data.Dataset\n",
        "\n",
        "  DUNDER METHODS:\n",
        "    - __len__(self)\n",
        "    - __getitem__(self, idx)\n",
        "\n",
        "  INSTANCE VARIABLES:\n",
        "    - train\n",
        "    - root_dir\n",
        "    - img_size\n",
        "    - df\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, train, root_dir, img_size):\n",
        "    \"\"\"\n",
        "    Initializes instance variables for given parameters\n",
        "\n",
        "    :param train: whether dataset is a train or evaluation dataset\n",
        "    :type: bool\n",
        "    :param root_dir: root directory of files\n",
        "    :type: str\n",
        "    :param img_size: desired length of one side of resized image\n",
        "    :type: int\n",
        "    :var df: reads and displays csv file as two-axis display\n",
        "    :type: DataFrame\n",
        "    \"\"\"\n",
        "    self.train = train\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(os.path.join(root_dir, 'Train.csv' if train else 'Test.csv'))\n",
        "    self.img_size = img_size\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns length of csv file corresponding to dataset\n",
        "\n",
        "    :return: length of csv file\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns image from dataset at specific index along with class label\n",
        "\n",
        "    :param idx: index of desired item to get\n",
        "    :type: int\n",
        "    :return: returns dictionary of image with corresponding label\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    image = cv2.imread(os.path.join(self.root_dir, self.df.iloc[idx][7]), cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "    sample = {'image': torch.tensor(image), 'label': self.df.iloc[idx][6]}\n",
        "    return sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITUhF2D5NRU-"
      },
      "source": [
        "### **Dataset Creation Function**\n",
        "The function reassigns global variables to the corresponding Dataset and DataLoader classes, creating shuffled training and evaluation datasets separated into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOdO6gNr4m0Z"
      },
      "source": [
        "def create_datasets():\n",
        "  \"\"\"\n",
        "  Reassigns global training variables to corresponding datasets and dataloaders\n",
        "  \"\"\"\n",
        "  global training_dataset\n",
        "  global testing_dataset\n",
        "  global train_dataloader\n",
        "  global test_dataloader\n",
        "\n",
        "  training_dataset = TrafficSignDataset(train=True, root_dir='traffic_sign_images', img_size=50)\n",
        "  testing_dataset = TrafficSignDataset(train=False, root_dir='traffic_sign_images', img_size=50)\n",
        "  \n",
        "  train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "  test_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbWHTZ4gNlN8"
      },
      "source": [
        "## **Model Training and Evaluation**\n",
        "The functions below train / evaluate the network given in the function parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2LBbxIkNtBs"
      },
      "source": [
        "### **Model Training Function**\n",
        "The function trains the model, printing loss for every epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCRmtcZj5c3D"
      },
      "source": [
        "def train_model(net):\n",
        "  \"\"\"\n",
        "  Iterates over training dataset, adjusting model gradients\n",
        "  Prints epoch number and corresponding loss after every epoch\n",
        "\n",
        "  :param net: CNN to be trained\n",
        "  :type net: torch.nn.Module\n",
        "  \"\"\"\n",
        "  for epoch in range(EPOCHS):\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "      batch_imgs, batch_lbls = batch[\"image\"].view(image_size) / 255.0, batch[\"label\"]\n",
        "      batch_labels = [0 for i in range(BATCH_SIZE)]\n",
        "      for label_idx, label in enumerate(batch_lbls):\n",
        "        batch_labels[label_idx] = label.item()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(batch_imgs.to(device))\n",
        "      loss = criterion(outputs, torch.tensor([label for label in batch_labels], device=device).long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzne8eUYNYfL"
      },
      "source": [
        "### **Model Evaluation Function**\n",
        "The function iterates through the evaluation dataset, evaluating the network response by comparing it to each image's class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kn17ASC5yr4"
      },
      "source": [
        "def evaluate_model(net):\n",
        "  \"\"\"\n",
        "  Iterates over evaluation dataset without adjusting model gradients, comparing model predicted class to true class\n",
        "  Prints accuracy for each class, raw class total correct, and total image accuracy and raw score\n",
        "\n",
        "  :param net: CNN to be trained\n",
        "  :type net: torch.nn.Module\n",
        "  \"\"\"\n",
        "  total_classes = {}\n",
        "  class_correct = {}\n",
        "  total_images = 0\n",
        "  total_correct = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "      test_image, test_label = batch['image'].view(image_size) / 255.0, batch['label'].item()\n",
        "      correct_class = test_label\n",
        "      test_image = test_image.to(device)\n",
        "      predicted_class = torch.argmax(net(test_image)[0])\n",
        "      \n",
        "      total_images += 1\n",
        "      total_classes[predicted_class.item()] = total_classes[predicted_class.item()] + 1 if predicted_class.item() in total_classes else 1\n",
        "      \n",
        "      if predicted_class == correct_class:\n",
        "        total_correct += 1\n",
        "        class_correct[correct_class] = class_correct[correct_class] + 1 if correct_class in class_correct else 1\n",
        "  print([f'Accuracy for {img_class}: {round(100 * class_correct[img_class] / total_classes[img_class], 3)}%' for img_class in class_correct])\n",
        "  print(f'Raw class score: {class_correct}')\n",
        "  print(f'Total images correct: {total_correct}, Total images: {total_images}, Total accuracy: {round(100 * total_correct / total_images, 3)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw0rDAGFN1eb"
      },
      "source": [
        "## **Main Script**\n",
        "The main script uses the GPU if available, creates the datasets, CNN, Adam optimizer, CrossEntropyLoss criterion with class weights, and trains and evaluates the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkt76LEHJ76"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "# Create both training and evaluation datasets\n",
        "create_datasets()\n",
        "\n",
        "# Calculate total number of images in training dataset\n",
        "total_images = len(training_dataset)\n",
        "class_count = []\n",
        "for folder in os.listdir(train_path):\n",
        "  if folder != '.DS_Store':\n",
        "    image_count = len([img for img in os.listdir(os.path.join(train_path, folder))])\n",
        "    class_count.append(image_count)\n",
        "\n",
        "# Calculate the class weights (due to unequal class image sizes)\n",
        "final_weights = torch.Tensor([1 - img_count/total_images for img_count in class_count]).to(device)\n",
        "# Create the CNN on the GPU\n",
        "conv_net = ConvNet().to(device)                        \n",
        "# Initializes optimizer and criterion\n",
        "optimizer = optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(weight=final_weights)\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(conv_net)\n",
        "evaluate_model(conv_net)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}