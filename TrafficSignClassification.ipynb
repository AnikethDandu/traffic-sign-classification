{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrafficSignClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR96FeFKx4e/6wWBKqxihI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnikethDandu/traffic-sign-classification/blob/main/TrafficSignClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHxpB-LV3zrH"
      },
      "source": [
        "Convolutional Neural Network class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zvNd8X31UI"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    # Input image: 32x32x3\n",
        "    self.PADDING_SIZE = 1\n",
        "    self.KERNEL_SIZE = 3\n",
        "    self.STRIDE = 1\n",
        "    self.POOL_SIZE = 2\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv2 = nn.Conv2d(32, 64, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv3 = nn.Conv2d(64, 128, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv4 = nn.Conv2d(128, 256, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.fc1 = nn.Linear(2304, 512)\n",
        "    self.fc2 = nn.Linear(512, 43)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv4(x)), self.POOL_SIZE)\n",
        "    x = x.flatten(start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKU0VKEt7laU"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class TrafficSignDataset(Dataset):\n",
        "  def __init__(self, train, root_dir, img_size):\n",
        "    self.train = train\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(os.path.join(root_dir, 'Train.csv' if train else 'Test.csv'))\n",
        "    self.img_size = img_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = cv2.imread(os.path.join(self.root_dir, self.df.iloc[idx][7]), cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "    label = self.df.iloc[idx][6]\n",
        "    sample = {'image': torch.tensor(image), 'label': self.df.iloc[idx][6]}\n",
        "    return sample\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJkt76LEHJ76",
        "outputId": "756dca56-a366-488a-b2ca-4bfd47dbf0c8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "training_data = []\n",
        "testing_data = []\n",
        "\n",
        "image_size = (-1, 3, 50, 50)\n",
        "\n",
        "training_dataset = TrafficSignDataset(train=True, root_dir='traffic_sign_images', img_size=50)\n",
        "testing_dataset = TrafficSignDataset(train=False, root_dir='traffic_sign_images', img_size=50)\n",
        "\n",
        "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "total_images = 0\n",
        "class_img_count = []\n",
        "for folder in os.listdir('traffic_sign_images/Train'):\n",
        "  if folder != '.DS_Store':\n",
        "    image_count = len([img for img in os.listdir(os.path.join('traffic_sign_images/Train', folder))])\n",
        "    class_img_count.append(image_count)\n",
        "    total_images += image_count\n",
        "\n",
        "FINAL_WEIGHTS = torch.Tensor([1 - img_count/total_images for img_count in class_img_count]).to(device)\n",
        "CNN = ConvNet().to(device)\n",
        "optimizer = optim.Adam(CNN.parameters(), lr=0.0015)\n",
        "criterion = nn.CrossEntropyLoss(weight=FINAL_WEIGHTS)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    batch_imgs, batch_lbls = batch[\"image\"].view(image_size) / 255.0, batch[\"label\"]\n",
        "    batch_labels = [0 for i in range(32)]\n",
        "    for label_idx, label in enumerate(batch_lbls):\n",
        "      batch_labels[label_idx] = label.item()\n",
        "\n",
        "    batch_imgs = batch_imgs\n",
        "    batch_labels = batch_labels\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = CNN(batch_imgs.to(device))\n",
        "    loss = criterion(outputs, torch.tensor([label for label in batch_labels], device=device).long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f'Epoch: {epoch + 1}, Loss: {loss}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  total_classes = {}\n",
        "  class_correct = {}\n",
        "  total_images = 0\n",
        "  total_correct = 0\n",
        "\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    test_image, test_label = batch['image'].view(image_size) / 255.0, batch['label'].item()\n",
        "    correct_class = test_label\n",
        "    test_image = test_image.to(device)\n",
        "    predicted_class = torch.argmax(CNN(test_image)[0])\n",
        "    \n",
        "    total_images += 1\n",
        "    total_classes[predicted_class.item()] = total_classes[predicted_class.item()] + 1 if predicted_class.item() in total_classes else 1\n",
        "    \n",
        "    if predicted_class == correct_class:\n",
        "      total_correct += 1\n",
        "      class_correct[correct_class] = class_correct[correct_class] + 1 if correct_class in class_correct else 1\n",
        "print([f'Accuracy for {img_class}: {round(100 * class_correct[img_class] / total_classes[img_class], 3)}%' for img_class in class_correct])\n",
        "print(f'Raw class score: {class_correct}')\n",
        "print(f'Total images correct: {total_correct}, Total images: {total_images}, Total accuracy: {round(100 * total_correct / total_images, 3)}%')\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.8086117506027222\n",
            "Epoch: 2, Loss: 0.248038649559021\n",
            "Epoch: 3, Loss: 0.2852267622947693\n",
            "Epoch: 4, Loss: 0.3402959406375885\n",
            "Epoch: 5, Loss: 0.15202629566192627\n",
            "Epoch: 6, Loss: 0.054856497794389725\n",
            "Epoch: 7, Loss: 0.21575146913528442\n",
            "Epoch: 8, Loss: 0.06401488929986954\n",
            "Epoch: 9, Loss: 0.045847516506910324\n",
            "Epoch: 10, Loss: 0.13893479108810425\n",
            "['Accuracy for 33: 87.168%', 'Accuracy for 9: 76.599%', 'Accuracy for 12: 96.679%', 'Accuracy for 10: 99.278%', 'Accuracy for 4: 77.715%', 'Accuracy for 13: 95.174%', 'Accuracy for 8: 75.714%', 'Accuracy for 19: 36.129%', 'Accuracy for 38: 95.559%', 'Accuracy for 25: 81.619%', 'Accuracy for 14: 85.567%', 'Accuracy for 1: 85.118%', 'Accuracy for 42: 96.923%', 'Accuracy for 27: 41.176%', 'Accuracy for 2: 74.565%', 'Accuracy for 35: 95.798%', 'Accuracy for 7: 80.916%', 'Accuracy for 5: 77.969%', 'Accuracy for 31: 82.06%', 'Accuracy for 18: 62.613%', 'Accuracy for 29: 42.574%', 'Accuracy for 23: 69.231%', 'Accuracy for 3: 63.636%', 'Accuracy for 0: 44.231%', 'Accuracy for 17: 96.845%', 'Accuracy for 28: 77.439%', 'Accuracy for 11: 76.421%', 'Accuracy for 34: 100.0%', 'Accuracy for 30: 68.269%', 'Accuracy for 37: 90.909%', 'Accuracy for 21: 80.597%', 'Accuracy for 24: 66.667%', 'Accuracy for 20: 79.787%', 'Accuracy for 39: 71.605%', 'Accuracy for 26: 55.696%', 'Accuracy for 41: 64.706%', 'Accuracy for 40: 63.636%', 'Accuracy for 16: 92.308%', 'Accuracy for 36: 83.212%', 'Accuracy for 15: 91.935%', 'Accuracy for 6: 95.349%', 'Accuracy for 22: 61.654%', 'Accuracy for 32: 65.217%']\n",
            "Number of classes: 43\n",
            "Raw class score: {33: 197, 9: 455, 12: 524, 10: 550, 4: 551, 13: 710, 8: 318, 19: 56, 38: 624, 25: 373, 14: 249, 1: 612, 42: 63, 27: 28, 2: 686, 35: 342, 7: 318, 5: 499, 31: 247, 18: 278, 29: 43, 23: 72, 3: 371, 0: 23, 17: 307, 28: 127, 11: 363, 34: 111, 30: 71, 37: 50, 21: 54, 24: 26, 20: 75, 39: 58, 26: 44, 41: 33, 40: 42, 16: 144, 36: 114, 15: 171, 6: 123, 22: 82, 32: 60}\n",
            "Total images: 12630\n",
            "Total accuracy: 81.108%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgAG3K6-u43"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp -r /content/gdrive/My\\ Drive/ColabNotebooks/Data/ traffic_sign_images.zip\n",
        "!unzip traffic_sign_images.zip/traffic_sign_images.zip\n",
        "!rm -r traffic_sign_images.zip/\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}