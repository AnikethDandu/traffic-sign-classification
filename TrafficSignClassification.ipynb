{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrafficSignClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML1ld3JEzCblkZpOc0bHNi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnikethDandu/traffic-sign-classification/blob/main/TrafficSignClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPE0QpmtL9y2"
      },
      "source": [
        "# **Traffic Sign Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDRPiq7M-y2"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ5biOPK3mdf"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCycgL3iNHHZ"
      },
      "source": [
        "## **Placeholder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCnIB_Hm47cd"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "image_size = (-1, 3, 50, 50)\n",
        "\n",
        "training_dataset = None\n",
        "testing_dataset = None\n",
        "train_dataloader = None\n",
        "test_dataloader = None\n",
        "\n",
        "train_path = 'traffic_sign_images/Train'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJrsqdxNKPH"
      },
      "source": [
        "## **Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zvNd8X31UI"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    # Input image: 32x32x3\n",
        "    self.PADDING_SIZE = 1\n",
        "    self.KERNEL_SIZE = 3\n",
        "    self.STRIDE = 1\n",
        "    self.POOL_SIZE = 2\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv2 = nn.Conv2d(32, 64, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv3 = nn.Conv2d(64, 128, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.conv4 = nn.Conv2d(128, 256, \n",
        "                           kernel_size=self.KERNEL_SIZE, \n",
        "                           stride=self.STRIDE, \n",
        "                           padding=self.PADDING_SIZE)\n",
        "    self.fc1 = nn.Linear(2304, 512)\n",
        "    self.fc2 = nn.Linear(512, 43)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), self.POOL_SIZE)\n",
        "    x = F.max_pool2d(F.relu(self.conv4(x)), self.POOL_SIZE)\n",
        "    x = x.flatten(start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Fq0ceYNOGS"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLxUiPzfNi2n"
      },
      "source": [
        "### **Custom Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKU0VKEt7laU"
      },
      "source": [
        "class TrafficSignDataset(Dataset):\n",
        "  def __init__(self, train, root_dir, img_size):\n",
        "    self.train = train\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(os.path.join(root_dir, 'Train.csv' if train else 'Test.csv'))\n",
        "    self.img_size = img_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = cv2.imread(os.path.join(self.root_dir, self.df.iloc[idx][7]), cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "    label = self.df.iloc[idx][6]\n",
        "    sample = {'image': torch.tensor(image), 'label': self.df.iloc[idx][6]}\n",
        "    return sample\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITUhF2D5NRU-"
      },
      "source": [
        "### **Dataset creation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOdO6gNr4m0Z"
      },
      "source": [
        "def create_datasets():\n",
        "  global training_dataset\n",
        "  global testing_dataset\n",
        "  global train_dataloader\n",
        "  global test_dataloader\n",
        "\n",
        "  training_dataset = TrafficSignDataset(train=True, root_dir='traffic_sign_images', img_size=50)\n",
        "  testing_dataset = TrafficSignDataset(train=False, root_dir='traffic_sign_images', img_size=50)\n",
        "  \n",
        "  train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "  test_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbWHTZ4gNlN8"
      },
      "source": [
        "## **Model Training and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2LBbxIkNtBs"
      },
      "source": [
        "### **Model Training Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCRmtcZj5c3D"
      },
      "source": [
        "def train_model(net):\n",
        "  for epoch in range(EPOCHS):\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "      batch_imgs, batch_lbls = batch[\"image\"].view(image_size) / 255.0, batch[\"label\"]\n",
        "      batch_labels = [0 for i in range(BATCH_SIZE)]\n",
        "      for label_idx, label in enumerate(batch_lbls):\n",
        "        batch_labels[label_idx] = label.item()\n",
        "        \n",
        "      batch_imgs = batch_imgs\n",
        "      batch_labels = batch_labels\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(batch_imgs.to(device))\n",
        "      loss = criterion(outputs, torch.tensor([label for label in batch_labels], device=device).long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss}')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzne8eUYNYfL"
      },
      "source": [
        "### **Model Evaluation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kn17ASC5yr4"
      },
      "source": [
        "def evaluate_model(net):\n",
        "  total_classes = {}\n",
        "  class_correct = {}\n",
        "  total_images = 0\n",
        "  total_correct = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "      test_image, test_label = batch['image'].view(image_size) / 255.0, batch['label'].item()\n",
        "      correct_class = test_label\n",
        "      test_image = test_image.to(device)\n",
        "      predicted_class = torch.argmax(net(test_image)[0])\n",
        "      \n",
        "      total_images += 1\n",
        "      total_classes[predicted_class.item()] = total_classes[predicted_class.item()] + 1 if predicted_class.item() in total_classes else 1\n",
        "      \n",
        "      if predicted_class == correct_class:\n",
        "        total_correct += 1\n",
        "        class_correct[correct_class] = class_correct[correct_class] + 1 if correct_class in class_correct else 1\n",
        "  print([f'Accuracy for {img_class}: {round(100 * class_correct[img_class] / total_classes[img_class], 3)}%' for img_class in class_correct])\n",
        "  print(f'Raw class score: {class_correct}')\n",
        "  print(f'Total images correct: {total_correct}, Total images: {total_images}, Total accuracy: {round(100 * total_correct / total_images, 3)}%')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw0rDAGFN1eb"
      },
      "source": [
        "## **Main Script**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkt76LEHJ76"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "create_datasets()\n",
        "\n",
        "total_images = 0\n",
        "class_count = []\n",
        "for folder in os.listdir(train_path):\n",
        "  if folder != '.DS_Store':\n",
        "    image_count = len([img for img in os.listdir(os.path.join(train_path, folder))])\n",
        "    class_count.append(image_count)\n",
        "    total_images += image_count\n",
        "\n",
        "final_weights = torch.Tensor([1 - img_count/total_images for img_count in class_count]).to(device)\n",
        "conv_net = ConvNet().to(device)\n",
        "optimizer = optim.Adam(conv_net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(weight=final_weights)\n",
        "\n",
        "train_model(conv_net)\n",
        "evaluate_model(conv_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glLQXmMN9RO"
      },
      "source": [
        "## **Google Drive Dataset Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgAG3K6-u43"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp -r /content/gdrive/My\\ Drive/ColabNotebooks/Data/ traffic_sign_images.zip\n",
        "!unzip traffic_sign_images.zip/traffic_sign_images.zip\n",
        "!rm -r traffic_sign_images.zip/\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}